---
marp: true
theme: gaia
paginate: true
math: katex
---

<style>
:root {
  font-family: "Fira Sans";
}

h1, h2 {
  text-align: center;
  font-size: 1.1em;
  color: #444;  
}

h1 {
  font-size: 1.2em;    
}

section {
  color: #222;
  /*background-color: white; */
}

b, strong {
   color: blue;
}

b, em {
   color: red;
}


.columns {
    display: grid;
    grid-template-columns: repeat(2, minmax(0, 1fr));
    gap: 1rem;
}

</style>    

<!-- _theme: reveal -->
<br><br>

## Bootstrap Aggregation (Bagging) and Random Forests

<center>

Instructor: Haoran LEI

Hunan University

</center>

---

## Pros of Tree-based Methods


- Trees are very easy to explain to people. In fact, they are
even easier to explain than linear regression!
- Some people believe that decision trees more closely mirror
human decision-making than do the regression and
classification approaches seen in previous chapters.
- Trees can be displayed graphically, and are easily
interpreted even by a non-expert (especially if they are
small).
- Trees can easily handle qualitative predictors without the
need to create dummy variables.


---

## From single-tree to many-trees

- However, trees generally do not have the same level of
predictive accuracy as some of the other regression (and
classification approaches) covered before.

-  By aggregating many decision trees, the predictive
performance of trees can be substantially improved. We
introduce these concepts next.

---

## Bagging

- _Bootstrap aggregation_, or _bagging_, is a general-purpose
procedure for reducing the variance of a statistical learning
method
    - it is particularly
useful and frequently used in the context of decision trees.

- Recall that given a set of $n$ independent observations
$Z_1, . . . , Z_n$, each with variance σ
2
, the variance of the mean
Z¯ of the observations is given by σ
2/n.

- In other words, averaging a set of observations reduces
variance. Of course, this is not practical because we
generally do not have access to multiple training sets.


